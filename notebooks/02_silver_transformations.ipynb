{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd53bb4",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    formats: ipynb,py:light\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"__file__\" in globals():\n",
    "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "else:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d76712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.spark_config import create_spark\n",
    "from pyspark.sql import functions as F, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = os.path.join(project_root, \"delta\", \"bronze\") + \"/\"\n",
    "silver_path = os.path.join(project_root, \"delta\", \"silver\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bronze_tables(spark):\n",
    "    \"\"\"Load all Bronze Delta tables into a dictionary.\"\"\"\n",
    "    return {\n",
    "        \"orders\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_orders_dataset\"),\n",
    "        \"order_items\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_order_items_dataset\"),\n",
    "        \"payments\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_order_payments_dataset\"),\n",
    "        \"customers\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_customers_dataset\"),\n",
    "        \"products\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_products_dataset\"),\n",
    "        \"sellers\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_sellers_dataset\"),\n",
    "        \"reviews\": spark.read.format(\"delta\").load(f\"{bronze_path}olist_order_reviews_dataset\"),\n",
    "        \"translations\": spark.read.format(\"delta\").load(f\"{bronze_path}product_category_name_translation\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77197276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df: DataFrame, subset_cols: list[str]) -> DataFrame:\n",
    "    \"\"\"Remove duplicates and null values based on subset columns.\"\"\"\n",
    "    return df.dropDuplicates().dropna(subset=subset_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7140703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_derived_columns(orders: DataFrame, order_items: DataFrame, payments: DataFrame):\n",
    "    \"\"\"Add calculated columns: total_price, profit_margin, delivery_time_days, payment_count.\"\"\"\n",
    "    order_items = (\n",
    "        order_items\n",
    "        .withColumn(\"total_price\", F.col(\"price\") + F.col(\"freight_value\"))\n",
    "        .withColumn(\"profit_margin\", F.col(\"price\") - F.col(\"freight_value\"))\n",
    "    )\n",
    "\n",
    "    orders = (\n",
    "        orders\n",
    "        .withColumn(\n",
    "            \"delivery_time_days\",\n",
    "            F.when(\n",
    "                F.col(\"order_delivered_customer_date\").isNotNull(),\n",
    "                F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_purchase_timestamp\"))\n",
    "            )\n",
    "        )\n",
    "        .filter(F.col(\"delivery_time_days\").isNotNull())  # remove undelivered\n",
    "        .filter(F.col(\"delivery_time_days\") > 0)          # remove invalid/zero\n",
    "    )\n",
    "\n",
    "    payments_agg = (\n",
    "        payments\n",
    "        .groupBy(\"order_id\")\n",
    "        .agg(F.sum(\"payment_installments\").alias(\"payment_count\"))\n",
    "    )\n",
    "\n",
    "    return orders, order_items, payments_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_tables(orders, customers, order_items, payments_agg, reviews, products, sellers, translations):\n",
    "    \"\"\"Join datasets to create enriched Silver tables.\"\"\"\n",
    "    silver_orders = (\n",
    "        orders\n",
    "        .join(customers, \"customer_id\", \"left\")\n",
    "        .join(payments_agg, \"order_id\", \"left\")\n",
    "        .join(reviews.select(\"order_id\", \"review_score\"), \"order_id\", \"left\")\n",
    "    )\n",
    "\n",
    "    silver_order_items = (\n",
    "        order_items\n",
    "        .join(products, \"product_id\", \"left\")\n",
    "        .join(sellers, \"seller_id\", \"left\")\n",
    "        .join(translations, \"product_category_name\", \"left\")\n",
    "    )\n",
    "\n",
    "    return silver_orders, silver_order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_delta(df: DataFrame, name: str):\n",
    "    \"\"\"Write DataFrame as a Delta table to the Silver path.\"\"\"\n",
    "    path = f\"{silver_path}{name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"Written {name} to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f786e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Pipeline entrypoint for Silver transformation.\"\"\"\n",
    "    spark = create_spark(\"Silver transformations\")\n",
    "    bronze = read_bronze_tables(spark)\n",
    "\n",
    "    # Clean each dataset\n",
    "    orders = clean_dataframe(bronze[\"orders\"], [\"order_id\"])\n",
    "    order_items = clean_dataframe(bronze[\"order_items\"], [\"order_id\", \"price\"])\n",
    "    payments = clean_dataframe(bronze[\"payments\"], [\"order_id\"])\n",
    "    customers = clean_dataframe(bronze[\"customers\"], [\"customer_id\"])\n",
    "    reviews = clean_dataframe(bronze[\"reviews\"], [\"order_id\"])\n",
    "    products = clean_dataframe(bronze[\"products\"], [\"product_id\"])\n",
    "    sellers = clean_dataframe(bronze[\"sellers\"], [\"seller_id\"])\n",
    "    translations = clean_dataframe(bronze[\"translations\"], [\"product_category_name\"])\n",
    "\n",
    "    # Add derived and aggregated columns\n",
    "    orders, order_items, payments_agg = add_derived_columns(orders, order_items, payments)\n",
    "\n",
    "    # Create enriched Silver datasets\n",
    "    silver_orders, silver_order_items = create_silver_tables(\n",
    "        orders, customers, order_items, payments_agg, reviews, products, sellers, translations\n",
    "    )\n",
    "\n",
    "    # Write to Delta\n",
    "    write_delta(silver_orders, \"orders_enriched\")\n",
    "    write_delta(silver_order_items, \"order_items_enriched\")\n",
    "\n",
    "\n",
    "    spark.stop()\n",
    "    print(\"\\nSilver transformation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01698702",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
